% \loop\iftrue%
% \errmessage{This article is copyrighted by ___ and should not be TeXed}%
% \repeat%

\documentclass{ifacconf}  % Comment this line out
\usepackage{natbib}        % required for bibliography

% \documentclass[a4paper, 10 pt, conference]{ieeeconf}  % Comment this line out
% \IEEEoverridecommandlockouts%
\pdfminorversion=4%

\input{preamble}
\input{math}
\input{symbols}

% \usepackage{flushend}% to equalize last page %%% use when text complete


\draft% show todos in red
% \final% give error if there is todos
% \comments% blank right column to make comments in pen & paper

% \newcommand{\id}{}
% \newcommand{\price}{31.00}
% \pubid{\begin{minipage}{\textwidth}\ \\[50pt]\centering
%   \copyright{} 20XX \_\_\_\_.\\
%   Personal use is permitted, but republication/redistribution requires permission.\\
%   % See https://www.ieee.org/publications/rights/index.html for more information.% if IEEE
% \end{minipage}
% }
% \makeglossaries

\begin{document}

\begin{frontmatter}
\title{\LARGE \bf
  Expectation-Maximization based defense mechanism for distributed Model Predictive Control
}

\author[First]{Rafael Accácio Nogueira}
\author[First]{Romain Bourdais}
\author[First]{Simon Leglaive}
\author[First]{Hervé Guéguen}
\address[First]{IETR-CentraleSupélec, 35510 Cesson-Sévigné, Ille-et-Vilaine, France\\
{\tt\small \{rafael-accacio.nogueira, romain.bourdais, simon.leglaive, herve.gueguen\}
@centralesupelec.fr}}

% \thanks{\centering The authors are with  \newline {\tt\small
% }\newline

% % <-this % stops a space
% \thanks{\centering The authors are with IETR-CentraleSupélec, 35510\newline Cesson-Sévigné, Ille-et-Vilaine, France \newline {\tt\small
% \{rafael-accacio.nogueira, romain.bourdais, herve.gueguen\}
% @centralesupelec.fr}\newline
% }%
% }

% \maketitle
% \IEEEpeerreviewmaketitle%
% \thispagestyle{empty}
% \pagestyle{empty}

\begin{abstract}% Abstract of not more than 250 words.
\end{abstract}

\begin{keyword}
  Model predictive and optimization-based control,
  Distributed control and estimation,
Large scale optimization problems
\end{keyword}

\end{frontmatter}

\section{INTRODUCTION}
\todo{Some citations:
  \cite{MaestreEtAl2021}

}
% % % \mpc{} \dmpc{}


% % ~\\

\emph{Notation:} In this paper, $\norm{\cdot}$ and $\norm{\cdot}_{F}$ represent the $\ell_{2}$ and Frobenius norms. $\norm{\vec{v}}_{Y}$ is the weighted norm $\norm{Y^{\frac{1}{2}}\vec{v}}$.
$\Proj^{\set{T}}(\cdot)$ is the Euclidean projection onto set $\set{T}$.
$\card{x}$ is the number of elements in $x$.
$\kron{}{}$ represents the Kronecker product.
$\vectorize{A}$ vectorizes matrix $A$.
$\hadamard{}{}$ represents the Hadamard product.
$n\mathbin{:}i\mathbin{:}j$ is a row vector builder with elements $\{n,n+i,\dots,n+mi\}$, where $m=\mathrm{truncate}(\frac{j-n}{i})$, and ${n\mathbin{:}j}$ is equal to ${n\mathbin{:}1\mathbin{:}j}$.
$[A,B]$ and $[A;B]$ are horizontal and vertical concatenation of matrices of adequate sizes.
$0_{m\times n}$ and $1_{m\times n}$ are ${m\times n}$ matrices filled with $0$ and $1$.
$\0$ and $\1$ are 0 and 1 filled vectors of adequate size.
$\indicator{x}$ is the indicator function returning $1$ if $x$ is true and $0$ otherwise.
$\preceq$ is the component-wise less or equal for vectors and matrix inequality for matrices in ${\symmetric=\setbuild{X:\R^{n\times n}}{X'=X}}$.
${\defpos=\setbuild{X:\symmetric}{X\succ 0}}$, ${\semidefpos=\setbuild{X:\symmetric}{X\succeq 0}}$.
$I_{c}$ is the identity matrix of size $c$.
$A^{\dagger}$ is the generalized inverse ${{(A^{T}A)}^{-1}A^{T}}$.
$|A|$ is the determinant of $A$.
A vector $\vec{v}_{i}$, correspond to the $i$-th agent, and these vectors can be stacked in a vector $\vec{v}$.
$\random{a},\random{A},\randomvec{a}$ are random scalar, matrix and vector.
$\expectation{\random{x}}$ is the expected value of $\random{x}$, and $\probability{A \mid B}$ is the conditional probability of $A$ given condition $B$.
${\diag(A_{1},\dots,A_{N})}$ corresponds to a block diagonal matrix.

\section{PRELIMINARIES AND PROBLEM STATEMENT}\label{sec:PS}
\begin{equation}
\begin{matrix}
  \label{eq:systems}
\vec{x}_{i}[k+1]=A_{i}\xik + B_{i}\uik
\end{matrix}
\end{equation}

\begin{equation}
  \label{eq:upositive}
  \uik\succeq\0
\end{equation}

\begin{equation}
  \label{eq:constraint}
  \sum^{\nsubsystems}_{i=1}\Gamma_{i}\uik\preceq\umax
\end{equation}
where ${\Gamma_{i}:\R^{n_{u}\times n_{u}}}$\todo{$\Gamma_{i}:\semidefpos$?}, ${\umax:\R^{n_{u}\times 1}}$ and ${n_{u}=\card{\uik}}$.

\begin{problem}{Global \mpc{} Problem.}\label{Pb:GOP}
\begin{equation*}
\begin{matrix}
\underset{\useq}{\mathrm{minimize}}&\resizebox{0.35\textwidth}{!}{$\overbrace{\sum\limits^{M}_{i=1} \overbrace{\sum_{j=1}^{\predhorz} \norm{\mpcvec{v}[i][k+j][k]}^{2}_{Q_i}+\norm{\mpcvec{u}[i][k+j-1][k]}^{2}_{R_i}}^{\textstyle{} \obji}}^{\textstyle{} \globobj}$}\\
\mathrm{subject~ to}&~\eqref{eq:systems},\eqref{eq:upositive}\ \mathrm{and}\ \eqref{eq:constraint}
\left\}\small
\begin{aligned}
  &\forall i\in \{1:\nsubsystems\}\\
  &\forall j\in \{1:\predhorz\}
\end{aligned}\right.

\end{matrix}
% \label{eq:dmpcModLOP}
\end{equation*}
where ${Q_{i}:\semidefpos}$ and ${R_{i}:\defpos}$, $\vik$ is a control objective.
Reference tracking can be written as ${\vik=\wik-\xik}$, where $\wik$ is a state reference.
Disturbance rejection can be written as ${\vik=\xik}$.

$\optglobobj$ denotes the optimal value of the problem~\ref{Pb:GOP}, and ${\optuseq}$ is the optimal control sequences for all agents.
The problem~\ref{Pb:GOP} is solved at each time $k$, and the ${\optuikk}$ are applied in each respective $i$ subsystem, following a receding horizon strategy.
\end{problem}

\subsection{Distributed Model Predictive Control}\label{ssec:dMPC}
\todo{\dmpc{},
}
\begin{subequations}
  \begin{equation}
    \left.
      \small
      \begin{aligned}
        J_{i}^{\star}(\thetaik)&=\underset{\uiseq}{\mathrm{minimize}} \obji\\
        \mathrm{s.t.} &\quad\eqref{eq:systems}\ \&\ \eqref{eq:upositive}\\
        &\quad\Gamma_{i}\uik\preceq\thetaik:\lambdaik\\
      \end{aligned}
    \right\}
    \small
    \begin{aligned}
      &\forall i\in \{1:\nsubsystems\}\\
      &\forall j\in \{1:\predhorz\}
    \end{aligned}
    \label{eq:DOP_local}
  \end{equation}
  \begin{equation}
    \small
    \begin{aligned}
      J^{\star}&=\underset{\thetaseq}{\mathrm{minimize}}\sum^{\nsubsystems}_{i=1} J^{\star}_i(\thetaik)\\
      \mathrm{s.t.} &\quad \sum_{i=1}^{\nsubsystems}\thetaik\preceq\umax
    \end{aligned}
    \label{eq:DOP_master}
  \end{equation}
\end{subequations}

\begin{equation}
  \label{eq:projectedSubgradient}
\vec{\theta}^{(p+1)}=\Proj^{\set{H}}(\vec{\theta}\p-\rho\p\vec{g}\p)
\end{equation}

Where ${\set{H} = \setbuild{\vec{\theta}}{\sum_{i=1}^{M}\vec{\theta}_{i}=\vec{u}_{\max}}}$, $\vec{\theta}$ are the allocations for all agents.
\todo{The sum $\sum_{i=1}^{M}\vec{\theta}_{i}$ can also be represented by the matrix multiplication $I_{c}^{M}\vec{\theta}$. Where ${c=N_{p}\pi_{\uik}}$.}


\begin{equation}
  \label{eq:thetaNegot}
\thetai\pplusone=\thetai\p+\rho\left(\vec{\lambda}_{i}^{\star} (\vec{\theta}_{i}\p)-I_{c}^{M} \pseudoinv{I_{c}^{M}}\vec{\lambda}^{\star} (\vec{\theta}\p)\right)
\end{equation}

where ${I_{c}^{M}=\kron{\1_{M,1}}{I_{c}}}$.
\todo{Observe that ${I_{c}^{M} \pseudoinv{I_{c}^{M}}\vec{v}=\frac{\sum_{i=0}^{M}\vec{v}_{i}}{M}}$}

\todo{
  But as the coordinator must also assure that ${\thetaik\succeq\0}$ so the local problems are feasible, we set 0 as lower bounds:
  }
  \todo[verify if need to use max]{
\begin{equation}
  % \label{eq:thetaNegot}
  \thetai\pplusone=
  \left\{ \thetai\p+\rho\left(\vec{\lambda}_{i}^{\star} (\vec{\theta}_{i}\p)-I_{c}^{M} \pseudoinv{I_{c}^{M}}\vec{\lambda}^{\star} (\vec{\theta}\p)\right)
  ,0\right\}
\end{equation}
}
\no{\todo[delete probably]{
  But observe in~\eqref{eq:projectedSubgradient} that if ${\vec{\theta}_{i}\p=\0}$, ${(\vec{\lambda}_{i}^{\star} (\vec{\theta}_{i}\p)-I_{c}^{M} \pseudoinv{I_{c}^{M}}\vec{\lambda}^{\star} (\vec{\theta}\p))}$ needs to be negative so $\vec{\theta}_{i}\pplusone$ becomes negative, which is impossible \question{is it though?} since $\lambdaik\succeq\0$, from the KKT optimality conditions, seen in~\cite[Chapter 5]{BoydVandenberghe2004}.
}}

% TODO(accacio): correct image
\begin{figure}[h]
  \centering
  \begin{tikzpicture}[node distance=1.7cm]
    \tikzset{
      agent/.style={draw,circle,minimum width=1cm,},
      coord/.style={draw,circle,minimum width=1.7cm,very thick}
    }
    % \draw[thick,blue,rounded corners=10pt] (-1.7,1.2) rectangle (1.7,-1.7);
    % \node at (1.2,-1.3) {O};
    % \node (pi) at (-2.5,.5) {$\pi$};
    % \node (phi) at (2.5,-1.) {$\phi_{c}$};

    \node[agent] (a1) {A$_{1}$};
    \node (a2) [right of=a1]  {$\dots$};
    \node[agent] (am) [right of=a2] {A$_{M}$};
    \node[coord] (c) [above of=a2] {Coord};


    \draw[-latex] (a1) to [out=60,in=210] node [sloped,above] {$\vec{\lambda_{1}}$} (c) ;
    \draw[-latex] (c) to [out=-120,in=30] node [sloped,below] {$\vec{\theta}_{1}$} (a1);

    \draw[-latex] (am) to [out=150,in=-60]node [sloped,below] {$\vec{\lambda}_{M}$} (c);
    \draw[-latex] (c) to [out=-30,in=120] node [sloped,above] {$\vec{\theta}_{M}$} (am);

  \end{tikzpicture}
  \caption{Graph representation of Coordinator exchangin with $M$ agents in a \dmpc{} framework.}\label{fig:dmpc_graph}
\end{figure}

\begin{algorithm2e}[h]
  \DontPrintSemicolon%
  $p:=0$\;
  Coordinator initializes $\vec{\theta}\p$ \;
  \Repeat{$\|\vec{\theta}^{(p)} -\vec{\theta}^{(p-1)}\|\leq\epsilon$}{
  Subsystems solve~\eqref{eq:DOP_local}, and send $\optlambdai (\thetai\p)$\;
  Coordinator updates allocations~\eqref{eq:thetaNegot}\;
  $p:=p+1$
}
 \caption{Quantity decomposition based \acrlong{dmpc}.}\label{alg:quantityAlg}
\end{algorithm2e}

\subsection{Quadratic Case --- Formal Analysis}\label{ssec:FA}
% \todo{As seen in \cite{NogueiraEtAl2021}},

\begin{equation}
\begin{matrix}
\underset{\Uik}{\mathrm{minimize}}& \overbrace{\frac{1}{2} \Uik^T H_i\Uik+{\fik}^T\Uik}^{\textstyle J_{i}(\vec{\theta}_{i})}\\
\mathrm{s.t.}&\Theta_{i}\Uik\preceq\vec{\theta}_{i}:\vec{\lambda}_{i}\\
&\Uik\succeq\0
\end{matrix}
\label{eq:quadratic_case}
\end{equation}

If we take reference tracking, for instance, we have:
\begin{equation}
\small\begin{matrix*}[l]
 H_i&=&\mathcal{D}_i^T\bar{Q}_i\mathcal{D}_i+\bar{R}_i\\
\fik&=&\mathcal{D}_i^T\bar{Q}_i(\mathcal{M}_i\xik-\Wik)\\

\end{matrix*}
\label{eq:matrices}
\end{equation}

We stack in vectors $\Uik$ and $\Wik$
the input and setpoint predictions sequences ${\uiseq}$ and ${\wiseq}$.
${\mathcal{M}_{i}}$  and ${\mathcal{D}_{i}}$ are the prediction matrices of the \mpc{}.
$\bar{Q}_{i}$, $\bar{R}_{i}$, and $\Theta_{i}$ are block diagonal matrices built repeating $\predhorz$ times $Q_{i}$, $R_{i}$, and $\Gamma_i$ respectively.
\todo{
  Observe that $H_{i}:\defpos$.
}

\todo{As seen in~\cite{BemporadEtAl2002} and in~\cite{BorrelliEtAl2017}, the solution for a constrained linear quadratic regulation problem yields in a \pwa{} function.
Similarly, we can get an explicit  solution for its dual variables $\vec{\lambda}_{i}$, which is also a \pwa{} function with respect to $\vec{\theta}_{i}$:}
\begin{equation}
  \begin{aligned}
    \label{eq:lambdafuntheta}
    \lambdai=
    \begin{cases}
      -P_{i}^{1}\vec{\theta}_{i}-\vec{s}_{i}^{1}[k]&\text{if}\ G_{1}[k]\thetai \preceq z_{1}[k] \\
      \qquad\quad \vdots&\qquad\quad \vdots\\
      -P_{i}^{N}\vec{\theta}_{i}-\vec{s}_{i}^{N}[k]&\text{if}\ G_{N}[k]\thetai \preceq z_{N}[k] \\
    \end{cases}
  \end{aligned}
\end{equation}
The halfspaces defined by the set of pairs $(G_{j}[k],z_{j}[k])$ represent a combination of active constraints for a given time $k$.

\begin{challenge}\label{ch:partition_unknown}
  It is important to note that since the actual state is unknown we cannot anticipate the partition of the space.
\end{challenge}
\begin{challenge}\label{ch:zone_unknown}
 For the same reason the values of $\vec{s}_{i}[k]$ are also unknown.
\end{challenge}

 As each constraint can be active or inactive, for a group of $n$ constraints, we can have at most $N=2^{n}$ different combinations of active/inactive sets and consequently $N$ regions (or zones).
 \begin{remark}\label{re:N_different_zones}
  Observe that depending on the constraints, some of these combinations can redundant or even make the optimization infeasible.
 \end{remark}
 \begin{assumption}
   We assume that none of these cases in Remark~\ref{re:N_different_zones} can occur, and we have $N$ zones.
 \end{assumption}

\todo{
  The values of $P_{i}^{j}$ and $\vec{s}_{i}^{j}$, depend on $H_{i}$, $\fik$ and parts of $\Theta_{i}$ which represents the active constraints.
  If we take for example when all constraints are active, we have ${P_{i}^{1}={(\Theta_{i}H_{i}^{-1}\Theta_{i}\T)}^{-1}}$ and ${\vec{s}_{i}^{1}[k]=P_{i}^{1}\Theta_{i}H_{i}^{-1}\fik}$.
  And if all constraints are inactive, ${P_{i}^{N}=0_{\card{\Theta_{i}}\times\card{\Theta_{i}}}}$ and ${\vec{s}_{i}^{N}=\0}$.
  Observe that in all functions, only $\vec{s}_{i}^{j}[k]$ depend on time $k$.
}

\section{Attack in DMPC scheme}\label{sec:attack}

\begin{equation}\label{eq:cheating}
\lambdaicheat=\gamma_{i}(\lambdai)
\end{equation}

% \subsection{Detection and mitigation}
\begin{assumption}
  $\gamma_{i}(\cdot)$ is the same during the negotiation phase for a given time $k$ (it does not depend on $p$).
\end{assumption}

\begin{assumption}
  equation
  T inversivel
  We suppose the agent chooses a linear function such as \[ {\tilde{\lambdai}=\gamma_{i}(\vec{\lambda}_{i})=\Tik\vec{\lambda}_{i}} \] resulting in:
  \begin{equation}
    \label{eq:linear_cheating}
    \tilde{\lambdai}=
        \begin{cases}
      -\Tik P_{i}^{1}\vec{\theta}_{i}-\Tik \vec{s}_{i}^{1}[k]&\text{if}\ G_{1}[k]\thetai \preceq z_{1}[k] \\
      \qquad\quad \vdots&\qquad\quad \vdots\\
      -\Tik P_{i}^{N}\vec{\theta}_{i}-\Tik \vec{s}_{i}^{N}[k]&\text{if}\ G_{N}[k]\thetai \preceq z_{N}[k] \\
    \end{cases}
  \end{equation}
and we define $\tilde{P}_{i}^{j}[k]=\Tik P_{i}^{j}$ and $\tilde{\vec{s}}_{i}^{j}[k]=\Tik\vec{s}_{i}^{j}[k]$.
\end{assumption}
\todo{
  Since $\Theta_{i}$ are diagonal we can assure\pdfnote{proof?} there exists one \question{zone?}zone where all constraints are active. We will fix $j=1$ for this zone and called it the $1$-zones.
  We can estimate the values for this partition using:
}
\begin{equation}
  \label{eq:lambdafuntheta_tilde}
\tilde{\lambdai}=\gamma_{i}(\lambdai(\thetai))=-\widehat{\tilde{P}_{i}^{1}}[k]\vec{\theta}_{i}-\widehat{\tilde{\vec{s}}_{i}^{1}}[k]
\end{equation}


\begin{assumption}\label{ass:Pnominal}
  The nominal value of $P_{i}^{1}$, denoted $\bar{P}_{i}^{1}$, is available from reliable attack-free historical data.
\end{assumption}

\todo{
  Using this strategy, we can detect a deviation from nominal behavior using ${E_{i}[k] =\|\widehat{\tilde{P}_{i}^{1}}[k]-\bar{P}_{i}^{1}\|_{F}}$, where ${\|\cdot\|_{F}}$ is the Frobenius norm.
  Let ${d_{i}\in\{0,1\}}$ be an indicator that detects the attack in agent $i$.
  If the disturbance $E_{i}[k]$ respects an arbitrary bound
}
\begin{equation}
  \label{eq:2}
  E_{i}[k]\leq\epsilon_{P},
\end{equation}
\todo{
  then  ${d_{i}=0}$, and no attack is detected. Otherwise, ${d_{i}=1}$, and a change in behavior of agent $i$ is detected.
}


\todo{
  Using these assumptions, we can try to estimate the inverse of $T_{i}(k)$ as in
}
\begin{equation}\label{eq:5}
\widehat{{T_{i}(k)}^{-1}}=\bar{P}_{i}^{1}{\widehat{\tilde{P}_{i}^{1}}[k]}^{-1},
\end{equation}

and from~\eqref{eq:lambdafuntheta}, we can derive a method to reconstruct $\vec{\lambda}_{i}$:
\begin{equation}
  \label{eq:lambdareconstruction}
  {\vec{\lambda}_{i}}_{\mathrm{rec}}=\widehat{{T_{i}[k]}^{-1}} \tilde{\vec{\lambda}_{i}}
\end{equation}
\subsection{Estimating}
\todo{
  In order to estimate $\widehat{\tilde{P}_{i}^{1}}$, we must have enough observed points in the $1$-zones. We propose to generate points surrounding arbitrary $\bar{\vec{\theta}_{i}}$ in the $1$-zones, and use them to estimate $\widehat{\tilde{P}_{i}^{1}}$. Now we have to find such points.
}

% TODO(accacio): change unconstrained notation
\todo{
  Let $\optuncUik$ be the solution of the unconstrained version of~\ref{eq:quadratic_case}.
  It is known that its solution is ${ {\optuncUik}=-H_{i}^{-1}\fik}$.
  For the constrained solution we know that the constraints will be inactive if they respect the inequalities $\Theta_{i}\optuncUik\preceq\thetai$.
  If they regard the inequalities, the constrained value is calculated as is, if not it should be projected onto the viable region.
  This way, for all constraints to be active, all inequalities should be disrespected in $\Theta_{i}\optuncUik\preceq\thetai$, thus \[\thetai\prec\Theta_{i}\optuncUik.\]
  % Since $H_{i}:\defpos$, then $H_{i}^{-1}:\defpos$,
  If we choose $\bar{\theta}_{i}=\0$, the constraints will be active when the  will only  that can assure that all constraints are active, then we can estimate ${\widehat{\tilde{P}_{i}^{1}}[k]}$ around this point. Unfortunately, since we don't know the hyperplanes that separates the different zones, some points generated for the estimation can belong to other zones.
  So we use the \EM{} algorithm to estimate the parameter for the $1$-zones, as it can clusterize the points and estimate parameters at the same time.
}

\section{Expectation-Maximization Algorithm}\label{sec:expect-maxim-algo}

The main objective of the \EM{} algorithm is to find maximum likelihood estimators for models with latent variables.

We will use our estimation problem to illustrate how the algorithm works, and
we will concentrate on the estimation of the parameters for a single agent in a single step $k$, so we drop the subscript $i$ and the time dependency $[k]$ to simplify the notation.

As in~\eqref{eq:lambdafuntheta},  any response variable $\tilde{\vec{\lambda}}$, is a function of an input $\vec{\theta}$, which belongs to a unknown zone ${\set{Z}=\{1\mathbin{:}Z\}}$, Challenge~\ref{ch:zone_unknown}.
The relationship between the input and the response is given by a set of parameters we want to estimate ${\set{P}=\setbuild{(\tilde{P}^{z},\tilde{\vec{s}}^{z})}{z\in\set{Z}}}$.

For an observation $o$, we have the input and response variables, identified as  ${\random{\vec{\theta}}_{o}}$ and ${\random{\vec{\lambda}}_{o}}$.
As~\eqref{eq:linear_cheating} gives us a multidimensional \pwa{} function, we propose to use a expansion of the model referred as \emph{mixture of switching regressions} in~\cite{QuandtRamsey1978} and \emph{mixture of linear regressions} in~\cite{FariaSoromenho2010}, which we will call \emph{mixture of switching affine regressions}, since our regressors have a linear term (matrices $\tilde{P}^{z}$) and a constant term (vectors $\tilde{\vec{s}}^{z}$):
% TODO(accacio): change subscript n
  \begin{equation}
    \label{eq:linear_cheating_random}
    \random{\lambda}_{o}=
        \begin{cases}
      -\tilde{P}^{1}\randomvec{\theta}_{o}-\tilde{\vec{s}}^{1}&\text{with probability}\ \pi_{1} \\
      \qquad\quad \vdots&\qquad\quad \vdots\\
      -\tilde{P}^{Z}\randomvec{\theta}_{o}-\tilde{\vec{s}}^{Z}&\text{with probability}\ \pi_{Z} \\
    \end{cases}
  \end{equation}

As we can see, this observation does not gives us the complete data, since we don't know which zone generated the response, but only the probability of the input being in that zone.
We consider the complete data consisting of the input, the response, and a non-observable variable. Therefore, for each observation $o$ there exists a latent random variable ${\random{z}_{o}\in\set{Z}}$, with associated probabilities ${\Pi=\{\pi_{1},\dots,\pi_{Z}\}}$ such prior probability
   \begin{equation}
     \label{eq:prob_zo_equal_z}
\probability{\random{z}_{o}=z}=\pi_{z}.
\end{equation}
Since $\vec{\theta}$ is our input, we know \emph{almost surely} their values giving
\begin{equation}
  \label{eq:theta_almost_surely}
  \probability{\randomvec{\theta}_{o}}=1.
\end{equation}
With this information we can use the graph in Fig.~\ref{fig:model} to represent the model.
\begin{figure}[b]
  \centering
  \begin{tikzpicture}
    \draw[thick,blue,rounded corners=10pt] (-1.7,1.2) rectangle (1.7,-1.7);
    \node at (1.2,-1.3) {O};
    \node (pi) at (-2.5,.5) {$\Pi$};
    \node (phi) at (2.5,-1.) {$\set{P}$};

    \graph [edge quotes={fill=white,inner sep=1pt},
    clockwise=3,nodes={circle,draw,rotate=-60,minimum width=1cm}] {
      a/"${\randomvec{\theta}_{o}}$"[rotate=60],b/"${\random{\vec{\lambda}}_{o}}$"[rotate=60],c/"${\random{z}}_{o}$"[rotate=60];
      {a,c} ->[-latex] b;
      (pi) ->[thick,{Circle[length=3.pt]}-latex] c;
      (phi) ->[thick,{Circle[length=3.pt]}-latex] b;
      };
  \end{tikzpicture}
  \caption{Graph representation of model proposed.}\label{fig:model}
\end{figure}

As said, the objective of the \EM{} algorithm is to maximize the likelihood, more precisely the complete data log-likelihood $\ln\probability{\random{\Theta},\random{\Lambda},\random{Z};\set{P}}$.
Using the dependency graph in Fig.~\ref{fig:model} yields
    \begin{equation}\label{eq:completedataLogLikelihood}
      \ln\probability{\random{\Theta},\random{\Lambda},\random{Z};\set{P}}=\ln \prod_{o=1}^{O}\probability{\randomvec{\theta}_{o},\randomvec{\lambda}_{o},\random{z}_{o};\set{P}} .
    \end{equation}
where the probability $\probability{\randomvec{\theta}_{o},\randomvec{\lambda}_{o},\random{z}_{o};\set{P}}$ is
\begin{equation}\label{eq:condional}
 \prod_{z=1}^{Z}{[\probability{\randomvec{\lambda}_{o}|\randomvec{\theta}_{o},\random{z}_{o}=z;\set{P}^{z}}\probability{\random{z}_{o}=z}\probability{\randomvec{\theta}_{o}}]}^{\indicator{\random{z}_{o}=z}},
\end{equation}
and $\set{P}^{z}$ is the $z$-th tuple in $\set{P}$.

For the likelihood $\probability{\randomvec{\lambda}_{o}|\randomvec{\theta}_{o},\random{z}_{o}=z,\set{P}^{z}}$, we use multivariate normal probability density functions
\begin{equation}
  \label{eq:multivariate_gaussian}
\mathcal{N}(\randomvec{\lambda}_{o};f(\randomvec{\theta}_{o};\set{P}^{z}),{\Sigma^{z}}),
\end{equation}
where the mean is
\[f(\randomvec{\theta}_{o};(P,\vec{s}))=-{P}\randomvec{\theta}_{o}-{\vec{s}}\]
and the covariance is ${\Sigma^{z}}$, which tends to $0$.

Using~\eqref{eq:prob_zo_equal_z},~\eqref{eq:theta_almost_surely}, and~\eqref{eq:multivariate_gaussian} in~\eqref{eq:completedataLogLikelihood} yields
\begin{equation}\label{eq:completedataLogLikelihood_complete}
\ln\probability{\random{\Theta},\random{\Lambda},\random{Z};\set{P}}=  \sum_{o=1}^{O}\sum_{z=1}^{Z}{\indicator{\random{z}_{o}=z}}
  \alpha_{zo},
\end{equation}
where ${\alpha_{zo}=\ln{\pi_{z}}+\ln{\mathcal{N}(\randomvec{\lambda}_{o};f(\randomvec{\theta}_{o};\set{P}^{z}),{\Sigma^{z}})}}$.

Now we can formalize the \EM{} problem.
\begin{problem}{Expectation Maximization Problem}\label{pb:EM}

  Given a set of observed data $(\random{\Theta},\random{\Lambda})$, estimate the unknown latent variables $\random{Z}$ and parameter set $\set{P}$ which maximize the complete-data log likelihood in~\eqref{eq:completedataLogLikelihood_complete}.
\end{problem}


But as mentioned, we do not observe the complete data $(\random{\Theta},\random{\Lambda},\random{Z})$, instead we observe only $(\random{\Theta},\random{\Lambda})$ and all information we can get from $\random{Z}$ is the posterior probability  ${\zeta(z_{zo};\set{P})=\probability{\random{z}_{o}=z|\randomvec{\lambda}_{o},\randomvec{\theta}_{o};\set{P}}}$, also called \emph{responsibility}, which can be calculated as
\begin{align}
  \label{eq:responsibilities}
\zeta(z_{zo};\set{P})&=\frac{\probability{\random{z}_{o}=z}\probability{\randomvec{\lambda}_{o}|\randomvec{\theta}_{o};\set{P}^{z}}}{\sum\limits_{j=1}^{Z}\probability{\random{z}_{o}=j}\probability{\randomvec{\lambda}_{o}|\randomvec{\theta}_{o};\set{P}^{j}}}\nonumber\\
  &=\frac{\pi_{z}{\mathcal{N}(\randomvec{\lambda}_{o};f(\randomvec{\theta}_{o};\set{P}^{z}),{\Sigma^{z}})}}{\sum\limits_{j=1}^{Z}\pi_{j}\mathcal{N}(\randomvec{\lambda}_{o};f(\randomvec{\theta}_{o};\set{P}^{j}),{\Sigma^{j}})}.
\end{align}

Observe that by taking
\begin{equation}\label{eq:argmaxz}
\underset{z_{o}}{\argmax}\ {\zeta(z_{zo};\set{P})},
\end{equation}
we can get the most probable $z$-zone which generated the observation $\randomvec{\lambda}_{o}$.


So, rather than using the complete data log-likelihood~\eqref{eq:completedataLogLikelihood_complete}, which we do not have, we use its expectation with respect to the prior calculated using a given set of parameter estimates $\set{P}_{\mathrm{cur}}$,  ${Q\left(\set{P},\set{P}_{\mathrm{cur}}\right)=\expectation[{\zeta(z_{zo};\set{P}_{\mathrm{cur}}}]{\ln\probability{\random{\Theta},\random{\Lambda},\random{Z};\set{P}}}}$:
\begin{equation}
  \label{eq:completedataLogLikelihood_expectation}
 Q\left(\set{P},\set{P}_{\mathrm{cur}}\right) =
  \sum_{o=1}^{O}\sum_{z=1}^{Z}  \zeta(z_{zo};\set{P}_{\mathrm{cur}})\alpha_{zo}
      ,
\end{equation}


Then we can find a new estimate of $\set{P}$ that maximizes $Q(\set{P},\set{P}_{\mathrm{cur}})$:
\begin{equation} \label{eq:Mstep}
  \set{P}_{\mathrm{new}}=\underset{\set{P}}{\argmax}\ Q(\set{P},\set{P}^{\mathrm{cur}}).
\end{equation}

With this information we can describe the \EM{} algorithm in two parts, first calculate the responsibilities $\zeta(z_{zo};\set{P}_{\mathrm{cur}})$, and then update the parameters.
Algorithm~\ref{alg:em} adpated from~\mbox{\cite[Chapter 9]{Bishop2006}} describes the steps.

\begin{algorithm2e}[h]
  \DontPrintSemicolon%
  Initialize parameters $\set{P}_{\mathrm{new}}$\;
  \Repeat{$\set{P}_{\mathrm{cur}}$ converge}{
    $\set{P}_{\mathrm{cur}}=\set{P}_{\mathrm{new}}$\;
  \textbf{E step} Evaluate responsibilities~\eqref{eq:responsibilities}\;
  \textbf{M step} Reestimate parameters~\eqref{eq:Mstep}\;
}
 \caption{Expectation Maximization}\label{alg:em}
\end{algorithm2e}

We can see some similarities to the K-planes algorithm (see~\cite{BradleyMangasarian2000}), but \EM{} is more compromising.
Instead of affecting the observed data to a cluster with 100\% of certainty (\emph{hard assignment}), \EM{} uses the responsibilities (\emph{soft assignment}) for each cluster.
Then, when we solve the \textbf{M Step}, rather than using only the data assigned to update the parameters of a certain cluster, we take into account all data, whose contribution to the update is weighted by their associated responsibilities.

Here we introduce a variable ${\phi^{z}={[\vectorize{\tilde{P}^{z}}^{T}\ {(\tilde{\vec{s}}^{z})}^{T} ]}^{T}}$ so we can calculate the new estimates of $\set{P}$ from~\eqref{eq:Mstep}. Using the KKT conditions~\cite{BoydVandenberghe2004}, we can find an optimal solution for the problem in~\eqref{eq:Mstep}, by
taking the gradients of~\eqref{eq:completedataLogLikelihood_expectation} with respect to vectors $\vec{\phi}^{z}$ and making them vanish.
Because of the multidimensional nature of the problem, some matrix operations are needed to synthesize the results.
After those operations, we have a matricial solution that yields the optimal estimates $\vec{\phi}_{z}^{\mathrm{new}}$:
\begin{equation}
  \label{eq:mstepestimation}
  \vec{\phi}_{z}^{\mathrm{new}}=\pseudoinv{(\Xi_{z}\random{\Omega})}\Xi_{z}\vectorize{\random{\Lambda}},
\end{equation}
where
${\random{\Omega}=[\hadamard{(\Upsilon \random{\Theta}\Delta)}{Y};G]}$,
with matrices
${\Upsilon=\kron{\1_{n}^{T}}{I_{n}}}$,
${\Delta=\kron{I_{N}}{\1_{n}^{T}}}$,
${Y=\kron{G}{\1_{n}}}$,
${G=\kron{\1_{N}^{T}}{I_{n}}}$,
and
\[{\Xi_{z}={\diag(\sqrt{{\zeta(z_{z1};\set{P}_{\mathrm{cur}})}}I_{n},\cdots,\sqrt{{\zeta(z_{zO};\set{P}_{\mathrm{cur}})}}I_{n})}}.\]

As we can see,~\eqref{eq:mstepestimation} is a weighted Least-Squares, that uses the responsibilities as weights, ignoring those values closer to $0$.

Once the estimates $\vec{\phi}_{z}^{\mathrm{new}}$ converge, we can reconstruct the estimates $\tilde{P}^{z}$ and $\tilde{\vec{s}}^{z}$, and use in our mitigation scheme proposed in \S\ref{sec:attack}.

\todo {
  Observe that not necessarily the parameters estimated associated to a value z corresponds to the z shown in
}
% \begin{equation*}
%   p(\randomvec{x})=\sum_{i=0}^{N}\pi_{i} \mathcal{N}(\random{y};f_{i}(\randomvec{x}),\Sigma_{i})
% \end{equation*}

% \begin{equation}
%   \mathcal{N}(\random{y}; {\mu}_{i},\Sigma_{i})=\frac{1}{{\sqrt{2\pi\sigma_{i}^{2}}}}e^{-\frac{(\random{y}-{\mu}_{i})^{2}}{2\sigma^{2}}}
% \end{equation}

% for a $D-$dimensional gaussian we have:
% \begin{equation}
%   \mathcal{N}(\randomvec{y}; \vec{\mu}_{i},\Sigma_{i})=\frac{1}{{{(2\pi)}^{D/2}}{|\Sigma_{i}|^{\frac{1}{2}}}}e^{-\frac{1}{2}\norm{\randomvec{y}-\vec{\mu}_{i}}^{2}_{\Sigma_{i}^{-1}}}
% \end{equation}

% \begin{figure}[b]
%   \centering
%   \scriptsize \def\svgwidth{0.49\textwidth}
%   \includegraphics[width=\columnwidth]{pwagaussian.pdf}
%   \caption{Example of Gaussian Probability density functions used in mixture for identification of a 2D Piecewise Affine function with 2 modes, where \todo[verify colormap]{black represents 0 probability and white maximum }}\label{fig:pwagaussian}
% \end{figure}
\todo{Simulated annealing as in \cite{OzerovFevotte2010}}

\section{Conclusion}

A conclusion section is not required. Although a conclusion may review
the main points of the paper, do not replicate the abstract as the
conclusion. A conclusion might elaborate on the importance of the work
or suggest applications and extensions.
\begin{ack}

\end{ack}

% \nocite{*}
\bibliography{bibliography}
% \todo[Change bibliography to the one used in the aux]{}
\end{document}
